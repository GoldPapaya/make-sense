{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install torch-geometric\n",
    "!pip install torch-scatter"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZelQpuVbkDa4",
    "outputId": "68156678-bfa5-4548-f30a-c189ad2efdf4"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
      "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import networkx as nx"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gjsxxxPkGBd",
    "outputId": "d51fbb81-917a-448a-830a-36b1e7af711c"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_edge_weight(synset_a, synset_b):\n",
    "    \"\"\"Calculate the edge weight between two synsets using Wu-Palmer similarity\"\"\"\n",
    "    try:\n",
    "        similarity = synset_a.wup_similarity(synset_b)\n",
    "        return similarity if similarity is not None else 0.0\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def get_neighbors(graph, node_name):\n",
    "    \"\"\"Get neighbors of any node (for testing purposes)\"\"\"\n",
    "    if node_name not in graph:\n",
    "        print(f\"Node '{node_name}' not found in the graph\")\n",
    "        return []\n",
    "    return list(graph.neighbors(node_name))\n",
    "\n",
    "class WordNetGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "        self.start_synset = None\n",
    "\n",
    "    def add_synset_node(self, synset, excluded_nodes_set=None):\n",
    "        \"\"\"Adds a synset node if it doesn't already exist and is not in the excluded set\"\"\"\n",
    "        if excluded_nodes_set and synset.name() in excluded_nodes_set:\n",
    "            return False\n",
    "        if synset.name() not in self.graph:\n",
    "            self.graph.add_node(synset.name(), synset=synset)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def add_edge_with_wup(self, synset_a, synset_b, excluded_nodes_set=None):\n",
    "        \"\"\"Adds an edge between two synsets using Wu-Palmer similarity as the weight,\n",
    "        only if both nodes are allowed and present in the graph\"\"\"\n",
    "        a_name = synset_a.name()\n",
    "        b_name = synset_b.name()\n",
    "        self.add_synset_node(synset_a, excluded_nodes_set=excluded_nodes_set)\n",
    "        self.add_synset_node(synset_b, excluded_nodes_set=excluded_nodes_set)\n",
    "\n",
    "        if a_name in self.graph and b_name in self.graph:\n",
    "            weight = calculate_edge_weight(synset_a, synset_b)\n",
    "            if weight > 0.0:\n",
    "                self.graph.add_edge(a_name, b_name, weight=weight)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def add_polysemy_edges(self, synset, excluded_nodes_set=None):\n",
    "        \"\"\"Connects edges to synonyms\"\"\"\n",
    "        lemma_name = synset.lemmas()[0].name()\n",
    "        all_senses = wn.synsets(lemma_name)\n",
    "        new_nodes_added = set()\n",
    "\n",
    "        for other_sense in all_senses:\n",
    "            if other_sense.name() == synset.name():\n",
    "                continue\n",
    "            if excluded_nodes_set and other_sense.name() in excluded_nodes_set:\n",
    "                continue\n",
    "            if self.add_edge_with_wup(synset, other_sense, excluded_nodes_set=excluded_nodes_set):\n",
    "                new_nodes_added.add(other_sense)\n",
    "        return new_nodes_added\n",
    "\n",
    "    def add_hierarchical_edges(self, synset, excluded_nodes_set=None):\n",
    "        \"\"\"Connects edges to direct hypernyms and hyponyms\"\"\"\n",
    "        related_synsets = synset.hypernyms() + synset.hyponyms()\n",
    "        new_nodes_added = set()\n",
    "\n",
    "        for neighbor in related_synsets:\n",
    "            if excluded_nodes_set and neighbor.name() in excluded_nodes_set:\n",
    "                continue\n",
    "            if self.add_edge_with_wup(synset, neighbor, excluded_nodes_set=excluded_nodes_set):\n",
    "                new_nodes_added.add(neighbor)\n",
    "        return new_nodes_added\n",
    "\n",
    "\n",
    "    def build_graph_from_synset(self, input_synset_str, depth, excluded_nodes_set=None):\n",
    "        \"\"\"Build a multi-relational graph using BFS\"\"\"\n",
    "        try:\n",
    "            self.start_synset = wn.synset(input_synset_str)\n",
    "        except nltk.corpus.wordnet.WordNetError:\n",
    "            print(f\"Error: Synset '{input_synset_str}' not found in WordNet.\")\n",
    "            return\n",
    "\n",
    "        self.graph.clear()\n",
    "        if excluded_nodes_set and self.start_synset.name() in excluded_nodes_set:\n",
    "            print(f\"Warning: Start synset '{input_synset_str}' is in the excluded set. Graph will be empty.\")\n",
    "            return\n",
    "\n",
    "        self.add_synset_node(self.start_synset, excluded_nodes_set=excluded_nodes_set)\n",
    "        if not self.graph.nodes:\n",
    "            return\n",
    "\n",
    "        to_process = {self.start_synset}\n",
    "        processed = {self.start_synset}\n",
    "        #print(f\"Building graph for '{input_synset_str}' to depth {depth}, excluding {len(excluded_nodes_set) if excluded_nodes_set else 0} nodes...\")\n",
    "        for current_depth in range(depth):\n",
    "            next_to_process = set()\n",
    "            for synset in to_process:\n",
    "                # apply synonym edges\n",
    "                poly_neighbors = self.add_polysemy_edges(synset, excluded_nodes_set=excluded_nodes_set)\n",
    "                # apply hypernym/hyponym edges\n",
    "                hiero_neighbors = self.add_hierarchical_edges(synset, excluded_nodes_set=excluded_nodes_set)\n",
    "                # combine all neighbors\n",
    "                new_neighbors = poly_neighbors.union(hiero_neighbors)\n",
    "\n",
    "                for neighbor in new_neighbors:\n",
    "                    if neighbor not in processed and (not excluded_nodes_set or neighbor.name() not in excluded_nodes_set):\n",
    "                        next_to_process.add(neighbor)\n",
    "\n",
    "            processed.update(next_to_process)\n",
    "            to_process = next_to_process\n",
    "            if not to_process:\n",
    "                break\n",
    "\n",
    "            #print(f\"Depth {current_depth+1} reached. Total nodes: {len(self.graph.nodes)}. Nodes to process next: {len(to_process)}\")\n",
    "        #print(f\"Graph build complete. Final Nodes: {len(self.graph.nodes)}, Final Edges: {len(self.graph.edges)}\")\n",
    "\n",
    "    def get_all_definitions(self):\n",
    "        \"\"\"Retrieves definitions for all nodes\"\"\"\n",
    "        definitions = {}\n",
    "        for node_name, data in self.graph.nodes(data=True):\n",
    "            synset = data['synset']\n",
    "            definitions[node_name] = f\"{synset.definition()} ({'; '.join(synset.examples())})\"\n",
    "        return definitions\n",
    "\n",
    "    def get_all_edges(self):\n",
    "        \"\"\"Retrieves a list of edges and their initial weights\"\"\"\n",
    "        edges = []\n",
    "        edge_attr = []\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            edges.append((u, v))\n",
    "            edge_attr.append(data['weight'])\n",
    "        return edges, edge_attr"
   ],
   "metadata": {
    "id": "vA52egYrkF-l"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_neighbors(graph, node_name):\n",
    "    if node_name not in graph:\n",
    "        print(f\"Warning: Node '{node_name}' not found in the graph.\")\n",
    "        return []\n",
    "    return list(graph.neighbors(node_name))"
   ],
   "metadata": {
    "id": "0B2pmCa3kF7h"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install sentence-transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YwbbHQCluzx",
    "outputId": "95a5c5b1-04e2-4d91-ca5c-156337bcc42b"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def get_node_features(graph_instance):\n",
    "    \"\"\"\n",
    "    Generates node features for the WordNet graph by encoding synset definitions.\n",
    "\n",
    "    Args:\n",
    "        graph_instance (WordNetGraph): An instance of the WordNetGraph class.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the node features (embeddings) for each synset.\n",
    "        dict: A mapping from synset name to its index in the feature tensor.\n",
    "    \"\"\"\n",
    "    definitions = graph_instance.get_all_definitions()\n",
    "    synset_names = sorted(definitions.keys())\n",
    "    definition_list = [definitions[name] for name in synset_names]\n",
    "    embeddings = sentence_model.encode(definition_list, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    # create a mapping from synset name to its index in the embeddings tensor\n",
    "    synset_to_idx = {name: i for i, name in enumerate(synset_names)}\n",
    "    return embeddings, synset_to_idx\n",
    "\n",
    "def get_synset_embedding(synset_name):\n",
    "    \"\"\"Get embedding for a single synset definition\"\"\"\n",
    "    try:\n",
    "        synset = wn.synset(synset_name)\n",
    "        definition = f\"{synset.definition()} ({'; '.join(synset.examples())})\"\n",
    "        embedding = sentence_model.encode([definition], convert_to_tensor=True, show_progress_bar=False)\n",
    "        return embedding\n",
    "    except (nltk.corpus.wordnet.WordNetError, KeyError):\n",
    "        return None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335,
     "referenced_widgets": [
      "0d6c29bf11554be8ad3f4c16bf00231d",
      "f63fb71f332c456b9e973c65284f3755",
      "f58ebe3a07a14e66a9335fc6ff8f3875",
      "96332abdafc94b92872a4d3aa69563af",
      "d3ffe32d91fe4e19bac86c174f8a246e",
      "6254a07ec61741e486e82991169489a8",
      "ca8de71bb0844dbb9ff5372514cbd425",
      "c939a8ca016f4db89568f1bcb2abd0b3",
      "b5abf4aa86184ed9a8f9a520f392013b",
      "a044cab071cd4d3596ea6bb82aaccffb",
      "bdd0720554eb4848a9d77e7dba9b15e2",
      "035d7e7424ba4d928cb759441aaaf1e7",
      "40dcf4b6395948f4b21dfb1d45481a09",
      "d4e823c413cb442098e19e69eb2c64d8",
      "e15f3d82d7d44042b8235849a6e7ce37",
      "6a8d3607ffe5475b9e202b551e620b79",
      "4c4b7e42fb704588a6317edc9922d02f",
      "f237c007fa5f4d5e89401eb4ad5fbb85",
      "785e174519a34f659fea2b028d81c0de",
      "00f5e97e22fa4325a5162a4eb294e0ef",
      "926eeadd745f40f0a051f1d70b74a8bd",
      "3c2d4edce7884a3ab9978efa4516afbc",
      "9f90cfe375b54ea7999403fb3c59eae2",
      "26b6190691fe484abc9fd58001e0b5c8",
      "e6ce26f354f649069d5fe31ad8d279e1",
      "43d808eacf0141049f1fe2d4f2bea98b",
      "22f3fe9960434c19a91976f15b4f009f",
      "99f4da4a7f48439fae36550e6cf2c9a9",
      "4c8b368e0f004166a9c935d25bda284d",
      "5133227a6b0a45f38f5fe092735e57b5",
      "52c10ccb714043dfbdbd621a0322dce9",
      "d0cd4057fef640388f751b755706ac8a",
      "9402a46bf35e48598639cedb2931d982",
      "aad70a84ed134fa49fe4fc928145efcb",
      "284ae546baac47fe9fe02d3857f75cdf",
      "4a919339eef142f299128cec1c8f8300",
      "d9bd399520e24937929011525697cff3",
      "9d7bcab72ea847d19b5106ed4295ab2a",
      "8656ca9e24e147379fb49b99d1da8e93",
      "adbd117d6131474e8f0532f04b3ba6f2",
      "4562631563704dbe8a0500fed199c1dd",
      "6152ff40d0274d489984d66ebb4436b2",
      "2f2ed477c7c24d5e8461f78bfdadf5b1",
      "6910bc5512c34e3fbe20966b39983870",
      "c154b0e3f19e4c6a86ee353dc593df03",
      "1d398782a440408cb8b65b5090493182",
      "edd33864cdca48c989fd7e7b5af149d2",
      "c3dfcfc50a484af3b5ade7a8a3a9f546",
      "d1abe51dc48547c78606639ff66718b6",
      "48ca08691f50423bb9f04e9e0b8151dc",
      "ad0c02fedd904b6ea2a3a04ec98494d5",
      "c8fc85ff07b144659797293d92513dfb",
      "f3575db9a3b444e6962fc994c51aa64a",
      "0a6ab3b34e754f518a860f9d8ab625a1",
      "791dd54f052c47b8912453f7b11b624c",
      "fa6d60f511e84faa9630548311caafc5",
      "9888f0da4b584e2c94ce6c7904770db0",
      "5d02a12bf5424b498c4656580c10ae90",
      "fce74ae422ff40aeab09450ab833d203",
      "75fe2e53f77b4e6686f55dca6bb34143",
      "876816c5658c40aabf2c2dd1f4c80686",
      "f1331b45720f4af1b6a5ec17d87b91e0",
      "99bf9448a47a4dc1b5ad3a5fea4fcef9",
      "e07d15444549496392dbb9241920bcd4",
      "34eca866288749d79ff3e10e02eded43",
      "7a8c3a3fab6345a1abaac269f69ff65e"
     ]
    },
    "id": "aw9_n6E0kzZQ",
    "outputId": "b1c5e97f-7d29-4171-fe19-3694d4e406b7"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d6c29bf11554be8ad3f4c16bf00231d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "035d7e7424ba4d928cb759441aaaf1e7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f90cfe375b54ea7999403fb3c59eae2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aad70a84ed134fa49fe4fc928145efcb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c154b0e3f19e4c6a86ee353dc593df03"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa6d60f511e84faa9630548311caafc5"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, edge_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        # GCN layer\n",
    "        self.conv1 = gnn.GCNConv(num_node_features, hidden_channels)\n",
    "        # linear layer\n",
    "        self.linear = nn.Linear(2 * hidden_channels + edge_dim, 1)\n",
    "        # (2 * hidden_channels for concatenated node features + edge_dim for edge attributes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # apply ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        row, col = edge_index\n",
    "        edge_features = torch.cat([x[row], x[col]], dim=1)\n",
    "        edge_features = torch.cat([edge_features, edge_attr], dim=1)\n",
    "\n",
    "        # apply linear layer to predict edge scores (logits)\n",
    "        edge_scores = self.linear(edge_features)\n",
    "        return edge_scores"
   ],
   "metadata": {
    "id": "apj8Y2eGkF1T"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "def generate_negative_samples(graph, num_negative_samples):\n",
    "    \"\"\"\n",
    "    Generates negative samples (non-existent edges) for a given graph.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The original NetworkX graph.\n",
    "        num_negative_samples (int): The number of negative samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple represents a non-existent edge (a, b).\n",
    "    \"\"\"\n",
    "    negative_samples = set()\n",
    "    nodes = list(graph.nodes())\n",
    "    num_nodes = len(nodes)\n",
    "    if num_nodes < 2:\n",
    "        return []\n",
    "\n",
    "    max_attempts = num_negative_samples * 10 # this prevents infinite looping\n",
    "    attempts = 0\n",
    "    while len(negative_samples) < num_negative_samples and attempts < max_attempts:\n",
    "        a = random.choice(nodes)\n",
    "        b = random.choice(nodes)\n",
    "        if a != b and not graph.has_edge(a, b) and not graph.has_edge(b, a):\n",
    "            ordered_edge = tuple(sorted((a, b)))\n",
    "            negative_samples.add(ordered_edge)\n",
    "        attempts += 1\n",
    "\n",
    "    return list(negative_samples)"
   ],
   "metadata": {
    "id": "A_4YYC5lkFh-"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#**Training Data Creation**\n",
    "import torch_geometric.data\n",
    "\n",
    "# 1. create an instance of WordNetGraph\n",
    "wn_graph_train = WordNetGraph()\n",
    "\n",
    "# 2. call build_graph_from_synset with entity.n.01\n",
    "train_synset = 'entity.n.01'\n",
    "wn_graph_train.build_graph_from_synset(train_synset, depth=4)\n",
    "train_nodes_set = set(wn_graph_train.graph.nodes()) # keep track of added nodes\n",
    "\n",
    "# 3. call the get_node_features function to obtain node embeddings and a mapping\n",
    "node_embeddings_train, synset_to_idx_train = get_node_features(wn_graph_train)\n",
    "\n",
    "# 4. retrieve all positive edges (a_name, b_name) and their original Wu-Palmer weights\n",
    "graph_edges_train_positive, graph_edge_weights_train_positive = wn_graph_train.get_all_edges()\n",
    "\n",
    "# 5. initialize lists for positive edge_index and corresponding input/target attributes\n",
    "edge_index_list_train_positive = []\n",
    "initial_edge_attr_train_input = []\n",
    "target_labels_train_positive = []\n",
    "for i, (a_name, b_name) in enumerate(graph_edges_train_positive):\n",
    "    a_idx = synset_to_idx_train[a_name]\n",
    "    b_idx = synset_to_idx_train[b_name]\n",
    "    edge_index_list_train_positive.append([a_idx, b_idx])\n",
    "    initial_edge_attr_train_input.append(graph_edge_weights_train_positive[i])\n",
    "    target_labels_train_positive.append(1.0) # positive label = 1\n",
    "\n",
    "# convert pos edges to tensors\n",
    "edge_index_train_positive = torch.tensor(edge_index_list_train_positive, dtype=torch.long).t().contiguous()\n",
    "initial_edge_attr_train_input = torch.tensor(initial_edge_attr_train_input, dtype=torch.float).unsqueeze(1)\n",
    "target_labels_train_positive = torch.tensor(target_labels_train_positive, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "# 6. generate negative samples\n",
    "num_negative_samples_train = len(graph_edges_train_positive)\n",
    "negative_samples_train = generate_negative_samples(wn_graph_train.graph, num_negative_samples_train)\n",
    "\n",
    "# 7. convert negative samples to edge_index format and create corresponding attributes\n",
    "edge_index_list_train_negative = []\n",
    "initial_edge_attr_train_negative = []\n",
    "target_labels_train_negative = []\n",
    "for a_name, b_name in negative_samples_train:\n",
    "    if a_name in synset_to_idx_train and b_name in synset_to_idx_train:\n",
    "        a_idx = synset_to_idx_train[a_name]\n",
    "        b_idx = synset_to_idx_train[b_name]\n",
    "        edge_index_list_train_negative.append([a_idx, b_idx])\n",
    "        initial_edge_attr_train_negative.append(0.0)\n",
    "        target_labels_train_negative.append(0.0) # negative label = 0\n",
    "\n",
    "# convert neg edge data to tensors\n",
    "edge_index_train_negative = torch.tensor(edge_index_list_train_negative, dtype=torch.long).t().contiguous()\n",
    "initial_edge_attr_train_negative = torch.tensor(initial_edge_attr_train_negative, dtype=torch.float).unsqueeze(1)\n",
    "target_labels_train_negative = torch.tensor(target_labels_train_negative, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "# 8. concatenate positive and negative samples for the final train_data object\n",
    "train_edge_index = torch.cat([edge_index_train_positive, edge_index_train_negative], dim=1)\n",
    "train_initial_edge_attr = torch.cat([initial_edge_attr_train_input, initial_edge_attr_train_negative], dim=0)\n",
    "train_target_labels = torch.cat([target_labels_train_positive, target_labels_train_negative], dim=0)\n",
    "\n",
    "train_data = torch_geometric.data.Data(\n",
    "    x=node_embeddings_train,\n",
    "    edge_index=train_edge_index,\n",
    "    edge_attr=train_initial_edge_attr,\n",
    "    y=train_target_labels\n",
    ")\n"
   ],
   "metadata": {
    "id": "f_u1fU3Bkc0j"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#**Evaluation Data Creation** (run optionally, for demo purposes)\n",
    "# 1. create an instance of WordNetGraph\n",
    "wn_graph_eval = WordNetGraph()\n",
    "\n",
    "# 2. call build_graph_from_synset with bank.n.01 for demo purposes\n",
    "eval_synset = 'bank.n.01'\n",
    "wn_graph_eval.build_graph_from_synset(eval_synset, depth=3, excluded_nodes_set=train_nodes_set)\n",
    "\n",
    "# 3. call the get_node_features function to obtain node embeddings and a mapping for evaluation data\n",
    "node_embeddings_eval, synset_to_idx_eval = get_node_features(wn_graph_eval)\n",
    "\n",
    "# 4. retrieve all positive edges (a_name, b_name) and their original Wu-Palmer weights\n",
    "graph_edges_eval_positive, graph_edge_weights_eval_positive = wn_graph_eval.get_all_edges()\n",
    "\n",
    "# 5. initialize lists for positive edge_index and corresponding input/target attributes\n",
    "edge_index_list_eval_positive = []\n",
    "initial_edge_attr_eval_input = []\n",
    "target_labels_eval_positive = []\n",
    "for i, (a_name, b_name) in enumerate(graph_edges_eval_positive):\n",
    "    if a_name in synset_to_idx_eval and b_name in synset_to_idx_eval:\n",
    "        a_idx = synset_to_idx_eval[a_name]\n",
    "        b_idx = synset_to_idx_eval[b_name]\n",
    "        edge_index_list_eval_positive.append([a_idx, b_idx])\n",
    "        initial_edge_attr_eval_input.append(graph_edge_weights_eval_positive[i])\n",
    "        target_labels_eval_positive.append(1.0) # positive label = 0\n",
    "\n",
    "# convert pos edge data to tensors\n",
    "if edge_index_list_eval_positive:\n",
    "    edge_index_eval_positive = torch.tensor(edge_index_list_eval_positive, dtype=torch.long).t().contiguous()\n",
    "    initial_edge_attr_eval_input = torch.tensor(initial_edge_attr_eval_input, dtype=torch.float).unsqueeze(1)\n",
    "    target_labels_eval_positive = torch.tensor(target_labels_eval_positive, dtype=torch.float).unsqueeze(1)\n",
    "else:\n",
    "    edge_index_eval_positive = torch.empty((2, 0), dtype=torch.long)\n",
    "    initial_edge_attr_eval_input = torch.empty((0, 1), dtype=torch.float)\n",
    "    target_labels_eval_positive = torch.empty((0, 1), dtype=torch.float)\n",
    "\n",
    "# 6. generate negative samples\n",
    "num_negative_samples_eval = len(graph_edges_eval_positive)\n",
    "negative_samples_eval = generate_negative_samples(wn_graph_eval.graph, num_negative_samples_eval)\n",
    "\n",
    "# 7. convert negative samples to edge_index format and create corresponding attributes\n",
    "edge_index_list_eval_negative = []\n",
    "initial_edge_attr_eval_negative = []\n",
    "target_labels_eval_negative = []\n",
    "for a_name, b_name in negative_samples_eval:\n",
    "    if a_name in synset_to_idx_eval and b_name in synset_to_idx_eval:\n",
    "        a_idx = synset_to_idx_eval[a_name]\n",
    "        b_idx = synset_to_idx_eval[b_name]\n",
    "        edge_index_list_eval_negative.append([a_idx, b_idx])\n",
    "        initial_edge_attr_eval_negative.append(0.0)\n",
    "        target_labels_eval_negative.append(0.0) # negative label = 0\n",
    "\n",
    "# convert neg edge data to tensors\n",
    "if edge_index_list_eval_negative:\n",
    "    edge_index_eval_negative = torch.tensor(edge_index_list_eval_negative, dtype=torch.long).t().contiguous()\n",
    "    initial_edge_attr_eval_negative = torch.tensor(initial_edge_attr_eval_negative, dtype=torch.float).unsqueeze(1)\n",
    "    target_labels_eval_negative = torch.tensor(target_labels_eval_negative, dtype=torch.float).unsqueeze(1)\n",
    "else:\n",
    "    edge_index_eval_negative = torch.empty((2, 0), dtype=torch.long)\n",
    "    initial_edge_attr_eval_negative = torch.empty((0, 1), dtype=torch.float)\n",
    "    target_labels_eval_negative = torch.empty((0, 1), dtype=torch.float)\n",
    "\n",
    "# 8. concatenate positive and negative samples for the final eval_data object\n",
    "eval_edge_index = torch.cat([edge_index_eval_positive, edge_index_eval_negative], dim=1)\n",
    "eval_initial_edge_attr = torch.cat([initial_edge_attr_eval_input, initial_edge_attr_eval_negative], dim=0)\n",
    "eval_target_labels = torch.cat([target_labels_eval_positive, target_labels_eval_negative], dim=0)\n",
    "\n",
    "eval_data = torch_geometric.data.Data(\n",
    "    x=node_embeddings_eval,\n",
    "    edge_index=eval_edge_index,\n",
    "    edge_attr=eval_initial_edge_attr,\n",
    "    y=eval_target_labels\n",
    ")\n"
   ],
   "metadata": {
    "id": "9I_3S1Nv3zyO"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_node_features = train_data.num_node_features\n",
    "hidden_channels = 64\n",
    "edge_dim = 1\n",
    "model = GNNModel(num_node_features, hidden_channels, edge_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.05\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "id": "NrHxEV1wkhig"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    predicted_edge_logits = model(train_data.x.clone(), train_data.edge_index, train_data.edge_attr)\n",
    "\n",
    "    # calculate loss using BCEWithLogitsLoss and the binary target labels (train_data.y)\n",
    "    loss = criterion(predicted_edge_logits, train_data.y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08jOa--Wkkmc",
    "outputId": "a9696072-2227-4e71-8d78-f966f3492a8c"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10/100, Loss: 0.28884828090667725\n",
      "Epoch 20/100, Loss: 0.19055433571338654\n",
      "Epoch 30/100, Loss: 0.14692918956279755\n",
      "Epoch 40/100, Loss: 0.11531563848257065\n",
      "Epoch 50/100, Loss: 0.09056276828050613\n",
      "Epoch 60/100, Loss: 0.07237529009580612\n",
      "Epoch 70/100, Loss: 0.05936029180884361\n",
      "Epoch 80/100, Loss: 0.049844641238451004\n",
      "Epoch 90/100, Loss: 0.042620811611413956\n",
      "Epoch 100/100, Loss: 0.036974336951971054\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_eval_edge_logits = model(eval_data.x.clone(), eval_data.edge_index, eval_data.edge_attr)\n",
    "\n",
    "eval_loss = criterion(predicted_eval_edge_logits, eval_data.y)\n",
    "predicted_probs = torch.sigmoid(predicted_eval_edge_logits)\n",
    "predicted_labels = (predicted_probs > 0.5).float()\n",
    "\n",
    "# convert tensors to numpy arrays for sklearn metrics\n",
    "true_labels = eval_data.y.cpu().numpy()\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "\n",
    "# metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels_np)\n",
    "\n",
    "print(f\"Evaluation Accuracy: {accuracy:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tv49Q97dkkj-",
    "outputId": "8951f20a-120f-4aa3-b946-608b433679b6"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Accuracy: 0.5007\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def apply_gnn_shift(eval_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_eval_edge_logits = model(eval_data.x.clone(), eval_data.edge_index, eval_data.edge_attr)\n",
    "\n",
    "    predicted_eval_edge_weights = torch.sigmoid(predicted_eval_edge_logits)\n",
    "    predicted_weights = [weight.item() for weight in predicted_eval_edge_weights]\n",
    "\n",
    "    return predicted_weights"
   ],
   "metadata": {
    "id": "aq3dZ7Mhkkhh"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def find_word_cost(wn_graph, start_node_name, target_node_name, predicted_edge_probabilities_list, graph_edges):\n",
    "    \"\"\"\n",
    "    Calculates the cost of the path from start_node_name to target_node_name\n",
    "    using the predicted GNN edge probabilities. The cost is the average of the inverted\n",
    "    probabilities (1 - probability) along the path.\n",
    "    \"\"\"\n",
    "    temp_graph = nx.Graph() # going to store a subgraph for the path\n",
    "\n",
    "    for node_name in wn_graph.graph.nodes():\n",
    "        temp_graph.add_node(node_name)\n",
    "\n",
    "    for i, (a_name, b_name) in enumerate(graph_edges):\n",
    "        probability = predicted_edge_probabilities_list[i]\n",
    "        cost = 1.0 - probability\n",
    "        temp_graph.add_edge(a_name, b_name, weight=cost)\n",
    "\n",
    "    if (start_node_name not in temp_graph.nodes) or (target_node_name not in temp_graph.nodes):\n",
    "        return float('inf')\n",
    "\n",
    "    try:\n",
    "        path_nodes = nx.shortest_path(temp_graph, source=start_node_name, target=target_node_name, weight='weight')\n",
    "\n",
    "        if len(path_nodes) <= 1:\n",
    "            return 0.0\n",
    "\n",
    "        path_costs = []\n",
    "        for j in range(len(path_nodes) - 1):\n",
    "            a = path_nodes[j]\n",
    "            b = path_nodes[j+1]\n",
    "            if temp_graph.has_edge(a, b):\n",
    "                path_costs.append(temp_graph[a][b]['weight'])\n",
    "\n",
    "        if path_costs:\n",
    "            average_path_cost = sum(path_costs)/len(path_costs)\n",
    "            return average_path_cost\n",
    "        else:\n",
    "            return float('inf')\n",
    "\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float('inf')\n",
    "    except Exception as e:\n",
    "        #print(f\"Error finding path between {start_node_name} and {target_node_name}: {e}\")\n",
    "        return float('inf')"
   ],
   "metadata": {
    "id": "IO7ferXfkva9"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_most_frequent_sense(word):\n",
    "    try:\n",
    "      return wn.synsets(word)[0]\n",
    "    except:\n",
    "      print('Sense ', word, ' not found in graph')\n",
    "\n",
    "def check_graph_for_sense(sense):\n",
    "    if sense.name() in set(wn_graph_eval.graph.nodes()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_all_senses(word):\n",
    "    try:\n",
    "        synsets = wn.synsets(word)\n",
    "        return [s.name() for s in synsets]\n",
    "    except:\n",
    "        print('Senses of ', word, ' not found in wordnet')"
   ],
   "metadata": {
    "id": "uiZuu98Xkxsg"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open('./sample_data/test_data.csv', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "data_io = io.StringIO(data)\n",
    "csv_reader = csv.reader(data_io)\n",
    "header = next(csv_reader)\n",
    "rows = list(csv_reader)\n",
    "df_original = pd.DataFrame(rows, columns=header)\n",
    "sentences = df_original['sentence/context'].tolist()\n",
    "words_to_disambiguate = df_original['polysemy_word'].tolist()\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    # remove any punctuation\n",
    "    cleaned_sentence = re.sub(r'[\\W_]+', ' ', sentence).lower()\n",
    "    tokens = cleaned_sentence.split()\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "wsd_senses = []\n",
    "for word in words_to_disambiguate:\n",
    "    wsd_senses.append(get_all_senses(word))\n",
    "\n",
    "print(\"Original DataFrame head:\")\n",
    "display(df_original.head())\n",
    "print(\"\\nSentences:\", sentences[:5]) # display first 5 for brevity\n",
    "print(\"Tokenized sentences:\", tokenized_sentences[:5]) # display first 5\n",
    "print(\"Words to disambiguate:\", words_to_disambiguate[:5]) # display first 5\n",
    "print(\"WSD senses (first entry):\", wsd_senses[0]) # display first entry"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "XWU3jVv6lCtu",
    "outputId": "a3407686-0ca0-433e-c1b3-49876d3ef9ea"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original DataFrame head:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "    \ufeffsn                          sentence/context polysemy_word\n",
       "0   345     His petition charged mental cruelty .       charged\n",
       "1   346     His petition charged mental cruelty .        mental\n",
       "2   347     His petition charged mental cruelty .       cruelty\n",
       "3  1405  One validated acts of school districts .           One\n",
       "4  1406  One validated acts of school districts .     validated"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-e3308316-9ca8-4d91-8f65-85645d4b0ac3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\ufeffsn</th>\n",
       "      <th>sentence/context</th>\n",
       "      <th>polysemy_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>charged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>mental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>cruelty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1405</td>\n",
       "      <td>One validated acts of school districts .</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1406</td>\n",
       "      <td>One validated acts of school districts .</td>\n",
       "      <td>validated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3308316-9ca8-4d91-8f65-85645d4b0ac3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e3308316-9ca8-4d91-8f65-85645d4b0ac3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e3308316-9ca8-4d91-8f65-85645d4b0ac3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-13d8ab91-4663-4792-ae75-401003af9fd5\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13d8ab91-4663-4792-ae75-401003af9fd5')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-13d8ab91-4663-4792-ae75-401003af9fd5 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"print(\\\"WSD senses (first entry):\\\", wsd_senses[0]) # display first entry\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"\\ufeffsn\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"346\",\n          \"1406\",\n          \"347\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence/context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"One validated acts of school districts .\",\n          \"His petition charged mental cruelty .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"polysemy_word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"mental\",\n          \"validated\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Sentences: ['His petition charged mental cruelty .', 'His petition charged mental cruelty .', 'His petition charged mental cruelty .', 'One validated acts of school districts .', 'One validated acts of school districts .']\n",
      "Tokenized sentences: [['his', 'petition', 'charged', 'mental', 'cruelty'], ['his', 'petition', 'charged', 'mental', 'cruelty'], ['his', 'petition', 'charged', 'mental', 'cruelty'], ['one', 'validated', 'acts', 'of', 'school', 'districts'], ['one', 'validated', 'acts', 'of', 'school', 'districts']]\n",
      "Words to disambiguate: ['charged', 'mental', 'cruelty', 'One', 'validated']\n",
      "WSD senses (first entry): ['charge.v.01', 'charge.v.02', 'charge.v.03', 'tear.v.03', 'appoint.v.02', 'charge.v.06', 'charge.v.07', 'charge.v.08', 'charge.v.09', 'commit.v.03', 'consign.v.02', 'charge.v.12', 'charge.v.13', 'agitate.v.02', 'charge.v.15', 'load.v.02', 'charge.v.17', 'charge.v.18', 'charge.v.19', 'charge.v.20', 'blame.v.03', 'charge.v.22', 'charge.v.23', 'charge.v.24', 'charge.v.25', 'charged.a.01', 'charged.s.02', 'aerated.s.02', 'charged.s.04']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch_geometric.data\n",
    "\n",
    "def build_sense_graph(sense, depth=3):\n",
    "    \"\"\"\n",
    "    sense: str of the synset sense (bank.n.01)\n",
    "    depth: int of the depth of the graph to build\n",
    "    \"\"\"\n",
    "    sense_graph = WordNetGraph()\n",
    "    sense_graph.build_graph_from_synset(sense, depth, excluded_nodes_set=train_nodes_set)\n",
    "    if not sense_graph.graph.nodes():\n",
    "        return [sense_graph, None, None]\n",
    "\n",
    "    node_embeddings, synset_to_idx = get_node_features(sense_graph)\n",
    "    graph_edges, graph_edge_weights = sense_graph.get_all_edges()\n",
    "\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "    for i, (a_name, b_name) in enumerate(graph_edges):\n",
    "        # Ensure nodes are still in the synset_to_idx map (they should be if the graph was built correctly)\n",
    "        if a_name in synset_to_idx and b_name in synset_to_idx:\n",
    "            a_idx = synset_to_idx[a_name]\n",
    "            b_idx = synset_to_idx[b_name]\n",
    "            edge_index_list.append([a_idx, b_idx])\n",
    "            edge_attr_list.append(graph_edge_weights[i])\n",
    "\n",
    "    if edge_index_list:\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    if edge_attr_list:\n",
    "        edge_attr = torch.tensor(edge_attr_list, dtype=torch.float).unsqueeze(1)\n",
    "    else:\n",
    "        edge_attr = torch.empty((0, 1), dtype=torch.float)\n",
    "\n",
    "    eval_data = torch_geometric.data.Data(x=node_embeddings, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return [sense_graph, eval_data, graph_edges]"
   ],
   "metadata": {
    "id": "R0rVBhfzlTIc"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Important: set start_row_index and end_row_index below."
   ],
   "metadata": {
    "id": "eJIGMRXGmrsI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define the slice for testing\n",
    "start_row_index = 0\n",
    "end_row_index = 5\n",
    "\n",
    "df_original_sliced = df_original.iloc[start_row_index:end_row_index].copy()\n",
    "df_original_sliced.reset_index(drop=True, inplace=True)\n",
    "sentences = df_original_sliced['sentence/context'].tolist()\n",
    "words_to_disambiguate = df_original_sliced['polysemy_word'].tolist()\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "    cleaned_sentence = re.sub(r'[\\W_]+', ' ', sentence).lower()\n",
    "    tokens = cleaned_sentence.split()\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "wsd_senses = []\n",
    "for word in words_to_disambiguate:\n",
    "    wsd_senses.append(get_all_senses(word))\n",
    "\n",
    "print(f\"Processing rows {start_row_index} to {end_row_index-1} from original data.\")\n",
    "print(\"Sliced sentences (first 5):\", sentences[:5])\n",
    "print(\"Sliced words to disambiguate (first 5):\", words_to_disambiguate[:5])\n",
    "print(\"Sliced WSD senses (first entry):\", wsd_senses[0])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgDCkqCXqKIN",
    "outputId": "41e29a8a-27af-454f-925a-5cc3885d0b44"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing rows 0 to 4 from original data.\n",
      "Sliced sentences (first 5): ['His petition charged mental cruelty .', 'His petition charged mental cruelty .', 'His petition charged mental cruelty .', 'One validated acts of school districts .', 'One validated acts of school districts .']\n",
      "Sliced words to disambiguate (first 5): ['charged', 'mental', 'cruelty', 'One', 'validated']\n",
      "Sliced WSD senses (first entry): ['charge.v.01', 'charge.v.02', 'charge.v.03', 'tear.v.03', 'appoint.v.02', 'charge.v.06', 'charge.v.07', 'charge.v.08', 'charge.v.09', 'commit.v.03', 'consign.v.02', 'charge.v.12', 'charge.v.13', 'agitate.v.02', 'charge.v.15', 'load.v.02', 'charge.v.17', 'charge.v.18', 'charge.v.19', 'charge.v.20', 'blame.v.03', 'charge.v.22', 'charge.v.23', 'charge.v.24', 'charge.v.25', 'charged.a.01', 'charged.s.02', 'aerated.s.02', 'charged.s.04']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# warning: this usually takes a long time, as it has to build all the graphs\n",
    "from tqdm import tqdm\n",
    "\n",
    "pre_built_graphs = {}\n",
    "all_unique_senses = set()\n",
    "for sense_list in wsd_senses:\n",
    "    for sense in sense_list:\n",
    "        all_unique_senses.add(sense)\n",
    "\n",
    "for sense in tqdm(all_unique_senses, desc=\"Building sense graphs\"):\n",
    "    res = build_sense_graph(sense, depth=3)\n",
    "    pre_built_graphs[sense] = res\n",
    "\n",
    "print(\"Finished pre-building graphs.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MV75DszGkkeo",
    "outputId": "b0540b28-f938-494f-e0ac-048a526c1929"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs:   4%|\u258d         | 2/51 [00:08<03:06,  3.81s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Start synset 'validate.v.04' is in the excluded set. Graph will be empty.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs:  16%|\u2588\u258c        | 8/51 [00:22<02:09,  3.02s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Start synset 'charge.v.08' is in the excluded set. Graph will be empty.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs:  27%|\u2588\u2588\u258b       | 14/51 [01:14<03:50,  6.23s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Start synset 'charge.v.03' is in the excluded set. Graph will be empty.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs:  35%|\u2588\u2588\u2588\u258c      | 18/51 [01:43<03:49,  6.94s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Start synset 'blame.v.03' is in the excluded set. Graph will be empty.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [02:30<01:38,  4.46s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Start synset 'charge.v.22' is in the excluded set. Graph will be empty.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 44/51 [03:14<00:05,  1.36it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Start synset 'charge.v.23' is in the excluded set. Graph will be empty.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 47/51 [03:18<00:05,  1.34s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Start synset 'charge.v.17' is in the excluded set. Graph will be empty.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building sense graphs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [03:52<00:00,  4.56s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished pre-building graphs.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd # Import pandas here to ensure it's available\n",
    "\n",
    "gnn_calculated_senses_output = [] # List to store ranked senses for each sentence\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    # print(f\"Sentence {i+1} complete\")\n",
    "    costs = []\n",
    "    best_senses_for_sentence = []\n",
    "    for sense in wsd_senses[i]:\n",
    "        sense_graph, sense_graph_tensors, graph_edges = pre_built_graphs[sense]\n",
    "        if sense_graph_tensors is None or not graph_edges:\n",
    "            total_cost_for_sense = float('inf')\n",
    "        else:\n",
    "            probabilities = apply_gnn_shift(sense_graph_tensors)\n",
    "            min_costs_per_token = []\n",
    "\n",
    "            for token in tokenized_sentences[i]:\n",
    "                if token == words_to_disambiguate[i]:\n",
    "                    continue\n",
    "                else:\n",
    "                    gnn_adjusted_costs_for_this_token = []\n",
    "                    wup_unadjusted_costs_for_this_token = []\n",
    "                    any_gnn_adjusted_cost_found = False\n",
    "\n",
    "                    token_senses = get_all_senses(token)\n",
    "                    for token_sense_name in token_senses:\n",
    "                        current_token_cost = find_word_cost(sense_graph, sense, token_sense_name, probabilities, graph_edges)\n",
    "                        if current_token_cost != float('inf'):\n",
    "                            gnn_adjusted_costs_for_this_token.append(current_token_cost)\n",
    "                            any_gnn_adjusted_cost_found = True\n",
    "                        else:\n",
    "                            try:\n",
    "                                main_sense_synset = wn.synset(sense)\n",
    "                                token_sense_synset = wn.synset(token_sense_name)\n",
    "                                wup_sim = calculate_edge_weight(main_sense_synset, token_sense_synset)\n",
    "                                if wup_sim > 0:\n",
    "                                    wup_unadjusted_costs_for_this_token.append(1.0 - wup_sim)\n",
    "                            except (nltk.corpus.wordnet.WordNetError, ValueError):\n",
    "                                pass\n",
    "                    if any_gnn_adjusted_cost_found:\n",
    "                        possible_token_costs = gnn_adjusted_costs_for_this_token\n",
    "                        indicator = \" *\"\n",
    "                    else:\n",
    "                        possible_token_costs = wup_unadjusted_costs_for_this_token\n",
    "                        indicator = \"\"\n",
    "                    # print(f'{indicator}Possible costs for token \"{token}\" with sense \"{sense}\": {possible_token_costs}')\n",
    "                    if possible_token_costs:\n",
    "                        min_costs_per_token.append(min(possible_token_costs))\n",
    "            if min_costs_per_token:\n",
    "                total_cost_for_sense = sum(min_costs_per_token) / len(min_costs_per_token)\n",
    "            else:\n",
    "                total_cost_for_sense = float('inf') # If no valid token costs, assign high cost\n",
    "\n",
    "        # print(f'Cost for sense {sense}: {total_cost_for_sense}')\n",
    "        costs.append(total_cost_for_sense)\n",
    "        best_senses_for_sentence.append((sense, total_cost_for_sense))\n",
    "\n",
    "    # print(f\"Total costs for sentence {i}: {sum(costs)}, individual sense costs: {costs}\")\n",
    "    print(f\"Sentence {i+1} complete\")\n",
    "\n",
    "    if best_senses_for_sentence:\n",
    "        # print(f\"Original sentence: {sentences[i]}\")\n",
    "        finite_cost_senses = [(s, c) for s, c in best_senses_for_sentence if c != float('inf')]\n",
    "        if finite_cost_senses:\n",
    "            ranked_senses = sorted(finite_cost_senses, key=lambda item: item[1], reverse=False) # sorts by lowest cost\n",
    "            senses_string = \", \".join([s for s, _ in ranked_senses])\n",
    "            gnn_calculated_senses_output.append(senses_string)\n",
    "        else:\n",
    "            # print(f\"No best sense with a finite cost found for sentence {i}\")\n",
    "            gnn_calculated_senses_output.append(\"\")\n",
    "    else:\n",
    "        # print(f\"No costs calculated for sentence {i}\")\n",
    "        gnn_calculated_senses_output.append(\"\")\n",
    "\n",
    "print(\"\\n--- GNN-adjusted WSD processing complete ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fM5eicllkkbm",
    "outputId": "614da45a-f117-4afe-9932-87af93d9d8dd"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 1 complete\n",
      "Sentence 2 complete\n",
      "Sentence 3 complete\n",
      "Sentence 4 complete\n",
      "Sentence 5 complete\n",
      "\n",
      "--- GNN-adjusted WSD processing complete ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "wup_calculated_senses_output = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    # print(f\"Sentence {i+1} complete\")\n",
    "    costs = []\n",
    "    best_senses_for_sentence = []\n",
    "    filtered_candidate_senses = [s for s in wsd_senses[i] if s not in train_nodes_set]\n",
    "    if not filtered_candidate_senses:\n",
    "        wup_calculated_senses_output.append(\"\")\n",
    "        continue\n",
    "\n",
    "    for sense in filtered_candidate_senses:\n",
    "        min_costs_per_token = []\n",
    "        for token in tokenized_sentences[i]:\n",
    "            if token == words_to_disambiguate[i]:\n",
    "                continue\n",
    "            else:\n",
    "                possible_token_costs = []\n",
    "                token_senses = get_all_senses(token)\n",
    "                for token_sense_name in token_senses:\n",
    "                    if token_sense_name in train_nodes_set:\n",
    "                        continue\n",
    "                    try:\n",
    "                        main_sense_synset = wn.synset(sense)\n",
    "                        token_sense_synset = wn.synset(token_sense_name)\n",
    "                        wup_sim = calculate_edge_weight(main_sense_synset, token_sense_synset)\n",
    "                        if wup_sim > 0:\n",
    "                            possible_token_costs.append(wup_sim)\n",
    "                    except (nltk.corpus.wordnet.WordNetError, ValueError):\n",
    "                        pass\n",
    "                # print(f'Possible costs for token \"{token}\" with sense \"{sense}\": {possible_token_costs} (Wu-Palmer unadjusted)')\n",
    "                if possible_token_costs:\n",
    "                    min_costs_per_token.append(min(possible_token_costs))\n",
    "        if min_costs_per_token:\n",
    "            total_cost_for_sense = sum(min_costs_per_token) / len(min_costs_per_token)\n",
    "        else:\n",
    "            total_cost_for_sense = 0\n",
    "        # print(f'Cost for sense {sense}: {total_cost_for_sense}')\n",
    "        costs.append(total_cost_for_sense)\n",
    "        best_senses_for_sentence.append((sense, total_cost_for_sense))\n",
    "    # print(f\"Total costs for sentence {i}: {sum(costs)}, individual sense costs: {costs}\")\n",
    "    print(f\"Sentence {i+1} complete\")\n",
    "    if best_senses_for_sentence:\n",
    "        # print(f\"Original sentence: {sentences[i]}\")\n",
    "        positive_cost_senses = [(s, c) for s, c in best_senses_for_sentence if c > 0]\n",
    "        if positive_cost_senses:\n",
    "            ranked_senses = sorted(positive_cost_senses, key=lambda item: item[1], reverse=True)\n",
    "            senses_string = \", \".join([s for s, _ in ranked_senses])\n",
    "            wup_calculated_senses_output.append(senses_string)\n",
    "        else:\n",
    "            # print(f\"No best sense with a positive Wu-Palmer similarity found for sentence {i}\")\n",
    "            wup_calculated_senses_output.append(\"\")\n",
    "    else:\n",
    "        # print(f\"No similarities calculated for sentence {i}\")\n",
    "        wup_calculated_senses_output.append(\"\")\n",
    "\n",
    "df_wup_results = df_original_sliced.copy() # Use the sliced DataFrame\n",
    "df_wup_results['calculated_senses'] = wup_calculated_senses_output\n",
    "\n",
    "for idx, row in df_wup_results.iterrows():\n",
    "    if not row['calculated_senses']:\n",
    "        df_wup_results.loc[idx, 'sentence/context'] = ''\n",
    "        df_wup_results.loc[idx, 'polysemy_word'] = ''\n",
    "\n",
    "print(\"\\n--- Wu-Palmer results saved to 'test_data_wup_results.csv' ---\")\n",
    "df_wup_results.to_csv('test_data_wup_results.csv', index=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJ7k20RhkkRl",
    "outputId": "525d7683-4416-4d4e-e4cb-548a42bb1645"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 1 complete\n",
      "Sentence 2 complete\n",
      "Sentence 3 complete\n",
      "Sentence 4 complete\n",
      "Sentence 5 complete\n",
      "\n",
      "--- Wu-Palmer results saved to 'test_data_wup_results.csv' ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Final Results Processing and Saving ---\n",
    "df_gnn_results = df_original_sliced.copy()\n",
    "df_wup_results = df_original_sliced.copy()\n",
    "df_gnn_results['calculated_senses'] = gnn_calculated_senses_output\n",
    "df_wup_results['calculated_senses'] = wup_calculated_senses_output\n",
    "\n",
    "for idx, row in df_gnn_results.iterrows():\n",
    "    gnn_senses_empty = not df_gnn_results.loc[idx, 'calculated_senses']\n",
    "    if gnn_senses_empty:\n",
    "        df_gnn_results.loc[idx, 'calculated_senses'] = ''\n",
    "        df_gnn_results.loc[idx, 'sentence/context'] = ''\n",
    "        df_gnn_results.loc[idx, 'polysemy_word'] = ''\n",
    "        df_wup_results.loc[idx, 'calculated_senses'] = ''\n",
    "        df_wup_results.loc[idx, 'sentence/context'] = ''\n",
    "        df_wup_results.loc[idx, 'polysemy_word'] = ''\n",
    "\n",
    "# save GNN-adjusted results\n",
    "print(\"\\n--- GNN-adjusted results saved to 'test_data_gnn_results.csv' ---\")\n",
    "df_gnn_results.to_csv('test_data_gnn_results.csv', index=False)\n",
    "display(df_gnn_results.head())\n",
    "\n",
    "# save Wu-Palmer only results\n",
    "print(\"\\n--- Wu-Palmer results saved to 'test_data_wup_results.csv' ---\")\n",
    "df_wup_results.to_csv('test_data_wup_results.csv', index=False)\n",
    "display(df_wup_results.head())\n",
    "\n",
    "print(\"\\nAll processing and saving complete\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "7yHhWHfOlpcF",
    "outputId": "c0875900-2770-4803-c32b-42688e9fcded"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- GNN-adjusted results saved to 'test_data_gnn_results.csv' ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "    \ufeffsn                          sentence/context polysemy_word  \\\n",
       "0   345     His petition charged mental cruelty .       charged   \n",
       "1   346     His petition charged mental cruelty .        mental   \n",
       "2   347     His petition charged mental cruelty .       cruelty   \n",
       "3  1405  One validated acts of school districts .           One   \n",
       "4  1406  One validated acts of school districts .     validated   \n",
       "\n",
       "                                   calculated_senses  \n",
       "0  charge.v.09, appoint.v.02, charged.a.01, charg...  \n",
       "1  mental.a.01, mental.a.02, mental.a.03, genial....  \n",
       "2           cruelty.n.01, cruelty.n.02, cruelty.n.03  \n",
       "3  one.s.01, one.s.02, one.s.03, one.s.04, one.s....  \n",
       "4  validated.s.01, validate.v.02, validate.v.03, ...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-7bbb52cc-7dec-495b-9d55-e2d3a4589c96\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\ufeffsn</th>\n",
       "      <th>sentence/context</th>\n",
       "      <th>polysemy_word</th>\n",
       "      <th>calculated_senses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>charged</td>\n",
       "      <td>charge.v.09, appoint.v.02, charged.a.01, charg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>mental</td>\n",
       "      <td>mental.a.01, mental.a.02, mental.a.03, genial....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>cruelty</td>\n",
       "      <td>cruelty.n.01, cruelty.n.02, cruelty.n.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1405</td>\n",
       "      <td>One validated acts of school districts .</td>\n",
       "      <td>One</td>\n",
       "      <td>one.s.01, one.s.02, one.s.03, one.s.04, one.s....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1406</td>\n",
       "      <td>One validated acts of school districts .</td>\n",
       "      <td>validated</td>\n",
       "      <td>validated.s.01, validate.v.02, validate.v.03, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bbb52cc-7dec-495b-9d55-e2d3a4589c96')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7bbb52cc-7dec-495b-9d55-e2d3a4589c96 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7bbb52cc-7dec-495b-9d55-e2d3a4589c96');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-ce901b95-18ce-4a0b-97a4-b4d84b3194ad\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce901b95-18ce-4a0b-97a4-b4d84b3194ad')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-ce901b95-18ce-4a0b-97a4-b4d84b3194ad button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"print(\\\"\\\\nAll processing and saving complete\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"\\ufeffsn\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"346\",\n          \"1406\",\n          \"347\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence/context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"One validated acts of school districts .\",\n          \"His petition charged mental cruelty .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"polysemy_word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"mental\",\n          \"validated\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"calculated_senses\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"mental.a.01, mental.a.02, mental.a.03, genial.a.02, mental.s.05\",\n          \"validated.s.01, validate.v.02, validate.v.03, validate.v.01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Wu-Palmer results saved to 'test_data_wup_results.csv' ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "    \ufeffsn                          sentence/context polysemy_word  \\\n",
       "0   345     His petition charged mental cruelty .       charged   \n",
       "1   346     His petition charged mental cruelty .        mental   \n",
       "2   347     His petition charged mental cruelty .       cruelty   \n",
       "3  1405  One validated acts of school districts .           One   \n",
       "4  1406  One validated acts of school districts .     validated   \n",
       "\n",
       "                                   calculated_senses  \n",
       "0  charge.v.02, appoint.v.02, charged.a.01, charg...  \n",
       "1  mental.a.01, mental.a.02, mental.a.03, genial....  \n",
       "2           cruelty.n.02, cruelty.n.03, cruelty.n.01  \n",
       "3  one.s.01, one.s.02, one.s.03, one.s.04, one.s....  \n",
       "4  validated.s.01, validate.v.02, validate.v.03, ...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-1170646b-d80e-4d42-aa86-fb9e9e779696\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\ufeffsn</th>\n",
       "      <th>sentence/context</th>\n",
       "      <th>polysemy_word</th>\n",
       "      <th>calculated_senses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>charged</td>\n",
       "      <td>charge.v.02, appoint.v.02, charged.a.01, charg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>mental</td>\n",
       "      <td>mental.a.01, mental.a.02, mental.a.03, genial....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347</td>\n",
       "      <td>His petition charged mental cruelty .</td>\n",
       "      <td>cruelty</td>\n",
       "      <td>cruelty.n.02, cruelty.n.03, cruelty.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1405</td>\n",
       "      <td>One validated acts of school districts .</td>\n",
       "      <td>One</td>\n",
       "      <td>one.s.01, one.s.02, one.s.03, one.s.04, one.s....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1406</td>\n",
       "      <td>One validated acts of school districts .</td>\n",
       "      <td>validated</td>\n",
       "      <td>validated.s.01, validate.v.02, validate.v.03, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1170646b-d80e-4d42-aa86-fb9e9e779696')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1170646b-d80e-4d42-aa86-fb9e9e779696 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1170646b-d80e-4d42-aa86-fb9e9e779696');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-ca36a468-a784-497e-ae38-a9a57494584e\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ca36a468-a784-497e-ae38-a9a57494584e')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-ca36a468-a784-497e-ae38-a9a57494584e button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"print(\\\"\\\\nAll processing and saving complete\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"\\ufeffsn\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"346\",\n          \"1406\",\n          \"347\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence/context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"One validated acts of school districts .\",\n          \"His petition charged mental cruelty .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"polysemy_word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"mental\",\n          \"validated\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"calculated_senses\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"mental.a.01, mental.a.02, mental.a.03, genial.a.02, mental.s.05\",\n          \"validated.s.01, validate.v.02, validate.v.03, validate.v.01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "All processing and saving complete\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}